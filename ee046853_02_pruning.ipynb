{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ee046853_02_pruning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2wOj/LJeQFdldzWKsPgyg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilshm/ee046853/blob/master/ee046853_02_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hXKfXmwTdYv",
        "colab_type": "text"
      },
      "source": [
        "# Technion - EE046853 - Advanced Topics in Computer Architecture\n",
        "In the previous part, you implemented a simple DNN for image classification. In this part, we'll discuss how the model can be pruned, i.e., how its memory footprint can theoretically be reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUpj1lwXTgIX",
        "colab_type": "text"
      },
      "source": [
        "# Part II: Model Pruning\n",
        "Pruning comes in many forms; however, it can be rougly divided into structured and unstructured pruning. We will employ the latter, as it is more straight-forward.\n",
        "Unstructured pruning involves reduction of parameters in granularities of a single weight or activations - we will only deal with static weights pruning.\n",
        "\n",
        "Obviously, we would like to remove weights which are the most redundant. The most intuitive way of doing so is throwing away weights which are smaller than a given threshold (it may be considered as local method, since the impact of the weights on the global loss function is not considered). This is usually done iteratively [1]: weights with magnitude below $\\sigma_1$ are pruned, the model is fine-tuned (i.e., trained) the recoup the loss in accuracy, then weights with magnitude below $\\sigma_2$, where $\\sigma_2 > \\sigma_1$, fine-tuning, and so on and so forth.\n",
        "In this exerecise we will ditch the fine-tuning part, for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDvyziJnVEAI",
        "colab_type": "text"
      },
      "source": [
        "## Magnitude Pruning\n",
        "We will not directly provide the code of doing so. However, here a few tips:\n",
        "\n",
        "1.   Each layer weights are stored at *LAYER.weights.data*.\n",
        "2.   It is a good practice *not* overriding the weights; therefore, create a new mask tensor with the same dimensions as the weights tensor (e.g., *torch.ones_like(LAYER.weights.data)*). After doing that, zero the values at the indices with small magnitudes.\n",
        "3.   Given a threshold $\\sigma$, masks can be easily created with boolean operations, for example \"<\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27YZQB49Ycuw",
        "colab_type": "text"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "1.   Prune your model (without retraining). Do not choose a threshold directly, but decide how many parameters you would like to prune (e.g., 10%) and then find the right threshold.\n",
        "2.   Show the model accuracy as a function of number of model parameters.\n",
        "3.   Choose one operating point and present the impact of pruning on each layer (e.g., how many parameters were pruned in each layer?).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4p7wOQlW6n4",
        "colab_type": "text"
      },
      "source": [
        "[1] Han, Song, et al. \"Learning both weights and connections for efficient neural network.\" Advances in neural information processing systems. 2015."
      ]
    }
  ]
}