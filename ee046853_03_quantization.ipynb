{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ee046853_03_quantization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNC8DhHvSdMvkwsZv432c4S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gilshm/ee046853/blob/master/ee046853_03_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcxLKRfYaDyn",
        "colab_type": "text"
      },
      "source": [
        "# Technion - EE046853 - Advanced Topics in Computer Architecture\n",
        "So far, we explored one vector to reduce DNN complexity - pruning.\n",
        "Yet another common vetor is quantization, i.e., represnting the model weights and/or activations values with less bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SykFLqwnbRWi",
        "colab_type": "text"
      },
      "source": [
        "# Part III: Model Quantization\n",
        "Reducing the number of bits represnting the DNN values has to major advantages.\n",
        "First, less bits equal to smaller memory footprint.\n",
        "Second, transforming the weights or activations (or both) to integers will simplify the multiply and accumulate operations.\n",
        "\n",
        "As always, quantization come in many forms. When dealing with hardware, it is common to discuss *uniform quantization*, since it can be practically implemented in hardware. The figure below presents the weights histogram of DNN layer and how it is quantized to 3bit (taken from [1]).\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/gilshm/ee046853/master/imgs/03_linear_quantization.png\" alt=\"Linear quantization\" width=\"300\"/></center>\n",
        "\n",
        "Representing a somewhat continuous range of values with small number of values will obviously lead to accuracy degradation.\n",
        "For simplicity, we will not deal with model fine-tuning after quantization. Fortunately, it is possible to quantize a model to 8b without significant loss in accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42WT80_Wdg8I",
        "colab_type": "text"
      },
      "source": [
        "## Symmetric Uniform Quantization\n",
        "In this exercise, we will quantize our weights in the most simplest way - which frankly is pretty common.\n",
        "Each quantized weight is\n",
        "\n",
        "$$\n",
        "w_{quantized} = \\text{Round}\\left(  \\frac{w}{ \\Delta }  \\right) \\cdot \\Delta \\,,\n",
        "$$\n",
        "\n",
        "where $\\Delta = \\frac{2 \\cdot \\text{max}(|w_{min}|, |w_{max}|)}{N} $, and $N = 2^{bits}$.\n",
        "Notice how we take the maximum value between the absolute values of $w_{min}$ and $w_{max}$ in order to obtain a symmetric quantization. For additional information, please check [2].\n",
        "\n",
        "Since we skip training, quantizing the weights is straightforward.\n",
        "In \"Part II\" we showed how the weights can be accessed. This time, instead of zeroing out some of them - quantize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7RZmcZTbX2I",
        "colab_type": "text"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "1.   Quantize your entire model weights (skip the first and the last layers) for different bits widths (8 to 2).\n",
        "2.   Show the model accuracy as a function of the bit widths.\n",
        "3.   Plot some histograms showing the layers before and after quantization.\n",
        "4.   Discuss how much memory may be saved for each quantization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7lK9nJ_brTu",
        "colab_type": "text"
      },
      "source": [
        "[1] Zhao, Ritchie, et al. \"Improving Neural Network Quantization without Retraining using Outlier Channel Splitting.\" International Conference on Machine Learning (2019).\n",
        "\n",
        "[2] Krishnamoorthi, Raghuraman. \"Quantizing deep convolutional networks for efficient inference: A whitepaper.\" arXiv preprint arXiv:1806.08342 (2018)."
      ]
    }
  ]
}